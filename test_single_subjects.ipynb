{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import prepare_data\n",
    "from train import get_dataloaders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from params import PARAMS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models import GCNAuto, GCRAM\n",
    "from main import init_model_params\n",
    "\n",
    "from train import train_model_2\n",
    "from main import model_predict, print_classification_report\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "SEQ_LEN = 400\n",
    "N_EPOCHS = 5000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "X_train shape: (5606348, 64)\n",
      "y_train shape: (5606348,)\n",
      "X_test shape: (590400, 64)\n",
      "y_test shape: (590400,)\n",
      "Shape with 2 classe (left, right arm):\n",
      "X_train shape: (2802428, 64)\n",
      "y_train shape: (2802428,)\n",
      "X_test shape: (295200, 64)\n",
      "y_test shape: (295200,)\n"
     ]
    }
   ],
   "source": [
    "from utils import load_data\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data('dataset/train/cross_subject_data_0.pickle')\n",
    "print('Original shapes:')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "X_train = X_train[np.isin(y_train, [0, 1])]\n",
    "y_train = y_train[np.isin(y_train, [0, 1])]\n",
    "X_test = X_test[np.isin(y_test, [0, 1])]\n",
    "y_test = y_test[np.isin(y_test, [0, 1])]\n",
    "\n",
    "print('Shape with 2 classe (left, right arm):')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "X_train = np.vstack([X_train, X_test])\n",
    "y_train = np.hstack([y_train, y_test])\n",
    "\n",
    "X_train, y_train = prepare_data(X_train, y_train, SEQ_LEN)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=PARAMS['TEST_SIZE'], shuffle=True, random_state=RANDOM_SEED)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=PARAMS['VALID_SIZE'], shuffle=True, random_state=RANDOM_SEED)\n",
    "dataloaders = get_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test, PARAMS['BATCH_SIZE'], random_seed=RANDOM_SEED, device=PARAMS['DEVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=GCRAM, opt=Adam(lr=0.001000), epochs=5000, device=cuda\n",
      "\n",
      "Epoch   1/5000, train loss: 0.7233, train acc: 0.4978, val loss: 0.6299, val acc: 0.5484\n",
      "Epoch  10/5000, train loss: 0.6867, train acc: 0.5483, val loss: 0.6294, val acc: 0.5609\n",
      "Epoch  20/5000, train loss: 0.6851, train acc: 0.5438, val loss: 0.6203, val acc: 0.5844\n",
      "Epoch  30/5000, train loss: 0.6862, train acc: 0.5563, val loss: 0.6250, val acc: 0.5672\n",
      "Epoch  40/5000, train loss: 0.6829, train acc: 0.5574, val loss: 0.6328, val acc: 0.5328\n",
      "Epoch  50/5000, train loss: 0.6834, train acc: 0.5625, val loss: 0.6233, val acc: 0.5797\n",
      "Epoch  60/5000, train loss: 0.6830, train acc: 0.5555, val loss: 0.6290, val acc: 0.5344\n",
      "Epoch  70/5000, train loss: 0.6812, train acc: 0.5599, val loss: 0.6262, val acc: 0.5797\n",
      "Epoch  80/5000, train loss: 0.6821, train acc: 0.5679, val loss: 0.6217, val acc: 0.6031\n",
      "Epoch  90/5000, train loss: 0.6820, train acc: 0.5654, val loss: 0.6219, val acc: 0.5719\n",
      "Epoch 100/5000, train loss: 0.6822, train acc: 0.5587, val loss: 0.6214, val acc: 0.5797\n",
      "Epoch 110/5000, train loss: 0.6830, train acc: 0.5533, val loss: 0.6245, val acc: 0.5594\n",
      "Epoch 120/5000, train loss: 0.6815, train acc: 0.5697, val loss: 0.6209, val acc: 0.5750\n",
      "Epoch 130/5000, train loss: 0.6767, train acc: 0.5703, val loss: 0.6206, val acc: 0.5875\n",
      "Epoch 140/5000, train loss: 0.6856, train acc: 0.5371, val loss: 0.6339, val acc: 0.5312\n",
      "Epoch 150/5000, train loss: 0.6778, train acc: 0.5671, val loss: 0.6185, val acc: 0.5813\n",
      "Epoch 160/5000, train loss: 0.6790, train acc: 0.5607, val loss: 0.6205, val acc: 0.5563\n",
      "Epoch 170/5000, train loss: 0.6748, train acc: 0.5759, val loss: 0.6108, val acc: 0.6016\n",
      "Epoch 180/5000, train loss: 0.6783, train acc: 0.5738, val loss: 0.6262, val acc: 0.5578\n",
      "Epoch 190/5000, train loss: 0.6805, train acc: 0.5719, val loss: 0.6202, val acc: 0.5703\n",
      "Epoch 200/5000, train loss: 0.6842, train acc: 0.5553, val loss: 0.6242, val acc: 0.5750\n",
      "Epoch 210/5000, train loss: 0.6843, train acc: 0.5539, val loss: 0.6294, val acc: 0.5578\n",
      "Epoch 220/5000, train loss: 0.6889, train acc: 0.5381, val loss: 0.6303, val acc: 0.5547\n",
      "Epoch 230/5000, train loss: 0.6863, train acc: 0.5458, val loss: 0.6327, val acc: 0.5469\n",
      "Epoch 240/5000, train loss: 0.6836, train acc: 0.5579, val loss: 0.6251, val acc: 0.5797\n",
      "Epoch 250/5000, train loss: 0.6815, train acc: 0.5630, val loss: 0.6207, val acc: 0.5687\n",
      "Epoch 260/5000, train loss: 0.6813, train acc: 0.5708, val loss: 0.6207, val acc: 0.5656\n",
      "Epoch 270/5000, train loss: 0.6828, train acc: 0.5609, val loss: 0.6256, val acc: 0.5672\n",
      "Epoch 280/5000, train loss: 0.6861, train acc: 0.5467, val loss: 0.6278, val acc: 0.5563\n",
      "Epoch 290/5000, train loss: 0.6852, train acc: 0.5577, val loss: 0.6242, val acc: 0.5687\n",
      "Epoch 300/5000, train loss: 0.6867, train acc: 0.5408, val loss: 0.6306, val acc: 0.5266\n",
      "Epoch 310/5000, train loss: 0.6820, train acc: 0.5702, val loss: 0.6213, val acc: 0.5891\n",
      "Epoch 320/5000, train loss: 0.6826, train acc: 0.5687, val loss: 0.6242, val acc: 0.5766\n",
      "Epoch 330/5000, train loss: 0.6816, train acc: 0.5684, val loss: 0.6238, val acc: 0.5609\n",
      "Epoch 340/5000, train loss: 0.6752, train acc: 0.5761, val loss: 0.6162, val acc: 0.5719\n",
      "Epoch 350/5000, train loss: 0.6764, train acc: 0.5813, val loss: 0.6179, val acc: 0.5906\n",
      "Epoch 360/5000, train loss: 0.6745, train acc: 0.5851, val loss: 0.6151, val acc: 0.5922\n",
      "Epoch 370/5000, train loss: 0.6737, train acc: 0.5872, val loss: 0.6206, val acc: 0.5984\n",
      "Epoch 380/5000, train loss: 0.6798, train acc: 0.5596, val loss: 0.6264, val acc: 0.5875\n",
      "Epoch 390/5000, train loss: 0.6761, train acc: 0.5818, val loss: 0.6167, val acc: 0.5938\n",
      "Epoch 400/5000, train loss: 0.6756, train acc: 0.5832, val loss: 0.6177, val acc: 0.6016\n",
      "Epoch 410/5000, train loss: 0.6772, train acc: 0.5674, val loss: 0.6208, val acc: 0.5734\n",
      "Epoch 420/5000, train loss: 0.6707, train acc: 0.5837, val loss: 0.6098, val acc: 0.5859\n",
      "Epoch 430/5000, train loss: 0.6746, train acc: 0.5743, val loss: 0.6145, val acc: 0.5797\n",
      "Epoch 440/5000, train loss: 0.6766, train acc: 0.5776, val loss: 0.6231, val acc: 0.5703\n",
      "Epoch 450/5000, train loss: 0.6739, train acc: 0.5837, val loss: 0.6156, val acc: 0.5828\n",
      "Epoch 460/5000, train loss: 0.6721, train acc: 0.5851, val loss: 0.6143, val acc: 0.5859\n",
      "Epoch 470/5000, train loss: 0.6792, train acc: 0.5687, val loss: 0.6218, val acc: 0.5563\n",
      "Epoch 480/5000, train loss: 0.6724, train acc: 0.5853, val loss: 0.6196, val acc: 0.5969\n",
      "Epoch 490/5000, train loss: 0.6710, train acc: 0.5883, val loss: 0.6185, val acc: 0.5844\n",
      "Epoch 500/5000, train loss: 0.6716, train acc: 0.5859, val loss: 0.6147, val acc: 0.5766\n",
      "Epoch 510/5000, train loss: 0.6697, train acc: 0.5920, val loss: 0.6187, val acc: 0.5828\n",
      "Epoch 520/5000, train loss: 0.6749, train acc: 0.5808, val loss: 0.6188, val acc: 0.5875\n",
      "Epoch 530/5000, train loss: 0.6791, train acc: 0.5773, val loss: 0.6213, val acc: 0.5766\n",
      "Epoch 540/5000, train loss: 0.6729, train acc: 0.5953, val loss: 0.6118, val acc: 0.6172\n",
      "Epoch 550/5000, train loss: 0.6726, train acc: 0.5837, val loss: 0.6231, val acc: 0.5500\n",
      "Epoch 560/5000, train loss: 0.6756, train acc: 0.5834, val loss: 0.6205, val acc: 0.5641\n",
      "Epoch 570/5000, train loss: 0.6761, train acc: 0.5907, val loss: 0.6259, val acc: 0.5656\n",
      "Epoch 580/5000, train loss: 0.6724, train acc: 0.5917, val loss: 0.6249, val acc: 0.5766\n",
      "Epoch 590/5000, train loss: 0.6700, train acc: 0.5885, val loss: 0.6219, val acc: 0.5891\n",
      "Epoch 600/5000, train loss: 0.6749, train acc: 0.5808, val loss: 0.6169, val acc: 0.5844\n",
      "Epoch 610/5000, train loss: 0.6739, train acc: 0.5845, val loss: 0.6232, val acc: 0.5531\n",
      "Epoch 620/5000, train loss: 0.6707, train acc: 0.5864, val loss: 0.6197, val acc: 0.5828\n",
      "Epoch 630/5000, train loss: 0.6716, train acc: 0.5831, val loss: 0.6146, val acc: 0.5828\n",
      "Epoch 640/5000, train loss: 0.6714, train acc: 0.5875, val loss: 0.6174, val acc: 0.5844\n",
      "Epoch 650/5000, train loss: 0.6708, train acc: 0.5861, val loss: 0.6175, val acc: 0.5750\n",
      "Epoch 660/5000, train loss: 0.6702, train acc: 0.5804, val loss: 0.6123, val acc: 0.6062\n",
      "Epoch 670/5000, train loss: 0.6660, train acc: 0.5957, val loss: 0.6124, val acc: 0.5938\n",
      "Epoch 680/5000, train loss: 0.6697, train acc: 0.5861, val loss: 0.6170, val acc: 0.5938\n",
      "Epoch 690/5000, train loss: 0.6711, train acc: 0.5792, val loss: 0.6169, val acc: 0.5859\n",
      "Epoch 700/5000, train loss: 0.6709, train acc: 0.5877, val loss: 0.6130, val acc: 0.5891\n",
      "Epoch 710/5000, train loss: 0.6704, train acc: 0.5965, val loss: 0.6087, val acc: 0.6109\n",
      "Epoch 720/5000, train loss: 0.6696, train acc: 0.5888, val loss: 0.6131, val acc: 0.6000\n",
      "Epoch 730/5000, train loss: 0.6714, train acc: 0.5923, val loss: 0.6176, val acc: 0.5828\n",
      "Epoch 740/5000, train loss: 0.6715, train acc: 0.5866, val loss: 0.6184, val acc: 0.6016\n",
      "Epoch 750/5000, train loss: 0.6735, train acc: 0.5891, val loss: 0.6154, val acc: 0.5859\n",
      "Epoch 760/5000, train loss: 0.6792, train acc: 0.5829, val loss: 0.6160, val acc: 0.5781\n",
      "Epoch 770/5000, train loss: 0.6782, train acc: 0.5788, val loss: 0.6112, val acc: 0.5891\n",
      "Epoch 780/5000, train loss: 0.6816, train acc: 0.5611, val loss: 0.6194, val acc: 0.5797\n",
      "Epoch 790/5000, train loss: 0.6813, train acc: 0.5643, val loss: 0.6184, val acc: 0.5813\n",
      "Epoch 800/5000, train loss: 0.6780, train acc: 0.5663, val loss: 0.6184, val acc: 0.5781\n",
      "Epoch 810/5000, train loss: 0.6774, train acc: 0.5757, val loss: 0.6169, val acc: 0.5797\n",
      "Epoch 820/5000, train loss: 0.6776, train acc: 0.5686, val loss: 0.6197, val acc: 0.5922\n",
      "Epoch 830/5000, train loss: 0.6730, train acc: 0.5781, val loss: 0.6200, val acc: 0.5906\n",
      "Epoch 840/5000, train loss: 0.6724, train acc: 0.5786, val loss: 0.6170, val acc: 0.5906\n",
      "Epoch 850/5000, train loss: 0.6735, train acc: 0.5802, val loss: 0.6185, val acc: 0.5609\n",
      "Epoch 860/5000, train loss: 0.6733, train acc: 0.5781, val loss: 0.6114, val acc: 0.5875\n",
      "Epoch 870/5000, train loss: 0.6708, train acc: 0.5893, val loss: 0.6158, val acc: 0.5953\n",
      "Epoch 880/5000, train loss: 0.6686, train acc: 0.6022, val loss: 0.6176, val acc: 0.6031\n",
      "Epoch 890/5000, train loss: 0.6717, train acc: 0.5885, val loss: 0.6101, val acc: 0.6219\n",
      "Epoch 900/5000, train loss: 0.6667, train acc: 0.5961, val loss: 0.6175, val acc: 0.6047\n",
      "Epoch 910/5000, train loss: 0.6728, train acc: 0.5890, val loss: 0.6089, val acc: 0.6031\n",
      "Epoch 920/5000, train loss: 0.6717, train acc: 0.5832, val loss: 0.6123, val acc: 0.6094\n",
      "Epoch 930/5000, train loss: 0.6711, train acc: 0.5915, val loss: 0.6173, val acc: 0.5938\n",
      "Epoch 940/5000, train loss: 0.6709, train acc: 0.5883, val loss: 0.6204, val acc: 0.5828\n",
      "Epoch 950/5000, train loss: 0.6704, train acc: 0.5923, val loss: 0.6103, val acc: 0.5891\n",
      "Epoch 960/5000, train loss: 0.6656, train acc: 0.6016, val loss: 0.6188, val acc: 0.5984\n",
      "Epoch 970/5000, train loss: 0.6724, train acc: 0.5855, val loss: 0.6082, val acc: 0.6109\n",
      "Epoch 980/5000, train loss: 0.6688, train acc: 0.5861, val loss: 0.6139, val acc: 0.5969\n",
      "Epoch 990/5000, train loss: 0.6732, train acc: 0.5796, val loss: 0.6256, val acc: 0.5719\n",
      "Epoch 1000/5000, train loss: 0.6735, train acc: 0.5725, val loss: 0.6246, val acc: 0.5422\n",
      "Epoch 1010/5000, train loss: 0.6746, train acc: 0.5740, val loss: 0.6234, val acc: 0.5922\n",
      "Epoch 1020/5000, train loss: 0.6712, train acc: 0.5826, val loss: 0.6226, val acc: 0.5719\n",
      "Epoch 1030/5000, train loss: 0.6706, train acc: 0.5896, val loss: 0.6170, val acc: 0.5859\n",
      "Epoch 1040/5000, train loss: 0.6723, train acc: 0.5821, val loss: 0.6224, val acc: 0.5750\n",
      "Epoch 1050/5000, train loss: 0.6725, train acc: 0.5850, val loss: 0.6225, val acc: 0.5859\n",
      "Epoch 1060/5000, train loss: 0.6753, train acc: 0.5858, val loss: 0.6228, val acc: 0.5719\n",
      "Epoch 1070/5000, train loss: 0.6692, train acc: 0.5961, val loss: 0.6112, val acc: 0.5922\n",
      "Epoch 1080/5000, train loss: 0.6729, train acc: 0.5835, val loss: 0.6228, val acc: 0.5750\n",
      "Epoch 1090/5000, train loss: 0.6717, train acc: 0.5856, val loss: 0.6110, val acc: 0.5797\n",
      "Epoch 1100/5000, train loss: 0.6705, train acc: 0.5942, val loss: 0.6142, val acc: 0.5891\n",
      "Epoch 1110/5000, train loss: 0.6686, train acc: 0.5945, val loss: 0.6159, val acc: 0.5734\n",
      "Epoch 1120/5000, train loss: 0.6720, train acc: 0.5941, val loss: 0.6140, val acc: 0.6016\n",
      "Epoch 1130/5000, train loss: 0.6697, train acc: 0.5985, val loss: 0.6117, val acc: 0.6141\n",
      "Epoch 1140/5000, train loss: 0.6717, train acc: 0.5938, val loss: 0.6155, val acc: 0.6109\n",
      "Epoch 1150/5000, train loss: 0.6707, train acc: 0.5945, val loss: 0.6182, val acc: 0.5938\n",
      "Epoch 1160/5000, train loss: 0.6710, train acc: 0.5824, val loss: 0.6158, val acc: 0.6031\n",
      "Epoch 1170/5000, train loss: 0.6701, train acc: 0.5883, val loss: 0.6154, val acc: 0.5969\n",
      "Epoch 1180/5000, train loss: 0.6717, train acc: 0.5894, val loss: 0.6161, val acc: 0.6094\n",
      "Epoch 1190/5000, train loss: 0.6733, train acc: 0.5789, val loss: 0.6200, val acc: 0.5984\n",
      "Epoch 1200/5000, train loss: 0.6710, train acc: 0.5812, val loss: 0.6186, val acc: 0.5672\n",
      "Epoch 1210/5000, train loss: 0.6780, train acc: 0.5748, val loss: 0.6228, val acc: 0.6062\n",
      "Epoch 1220/5000, train loss: 0.6737, train acc: 0.5824, val loss: 0.6214, val acc: 0.5906\n",
      "Epoch 1230/5000, train loss: 0.6774, train acc: 0.5805, val loss: 0.6242, val acc: 0.5719\n",
      "Epoch 1240/5000, train loss: 0.6769, train acc: 0.5692, val loss: 0.6195, val acc: 0.5813\n",
      "Epoch 1250/5000, train loss: 0.6763, train acc: 0.5781, val loss: 0.6183, val acc: 0.6047\n",
      "Epoch 1260/5000, train loss: 0.6746, train acc: 0.5773, val loss: 0.6183, val acc: 0.5891\n",
      "Epoch 1270/5000, train loss: 0.6775, train acc: 0.5808, val loss: 0.6197, val acc: 0.5969\n",
      "Epoch 1280/5000, train loss: 0.6758, train acc: 0.5813, val loss: 0.6236, val acc: 0.5641\n",
      "Epoch 1290/5000, train loss: 0.6709, train acc: 0.5856, val loss: 0.6143, val acc: 0.5922\n",
      "Epoch 1300/5000, train loss: 0.6699, train acc: 0.5840, val loss: 0.6193, val acc: 0.5719\n",
      "Epoch 1310/5000, train loss: 0.6711, train acc: 0.5858, val loss: 0.6136, val acc: 0.5891\n",
      "Epoch 1320/5000, train loss: 0.6729, train acc: 0.5941, val loss: 0.6120, val acc: 0.6031\n",
      "Epoch 1330/5000, train loss: 0.6736, train acc: 0.5826, val loss: 0.6165, val acc: 0.6109\n",
      "Epoch 1340/5000, train loss: 0.6744, train acc: 0.5654, val loss: 0.6189, val acc: 0.5797\n",
      "Epoch 1350/5000, train loss: 0.6712, train acc: 0.5823, val loss: 0.6132, val acc: 0.6172\n",
      "Epoch 1360/5000, train loss: 0.6728, train acc: 0.5772, val loss: 0.6198, val acc: 0.5844\n",
      "Epoch 1370/5000, train loss: 0.6736, train acc: 0.5802, val loss: 0.6214, val acc: 0.5813\n",
      "Epoch 1380/5000, train loss: 0.6746, train acc: 0.5780, val loss: 0.6151, val acc: 0.6078\n",
      "Epoch 1390/5000, train loss: 0.6744, train acc: 0.5851, val loss: 0.6092, val acc: 0.6281\n",
      "Epoch 1400/5000, train loss: 0.6770, train acc: 0.5733, val loss: 0.6155, val acc: 0.5922\n",
      "Epoch 1410/5000, train loss: 0.6753, train acc: 0.5729, val loss: 0.6168, val acc: 0.5953\n",
      "Epoch 1420/5000, train loss: 0.6701, train acc: 0.5866, val loss: 0.6218, val acc: 0.5906\n",
      "Epoch 1430/5000, train loss: 0.6711, train acc: 0.5880, val loss: 0.6160, val acc: 0.6109\n",
      "Epoch 1440/5000, train loss: 0.6729, train acc: 0.5869, val loss: 0.6135, val acc: 0.5984\n",
      "Epoch 1450/5000, train loss: 0.6756, train acc: 0.5733, val loss: 0.6238, val acc: 0.5922\n",
      "Epoch 1460/5000, train loss: 0.6759, train acc: 0.5832, val loss: 0.6174, val acc: 0.6078\n",
      "Epoch 1470/5000, train loss: 0.6775, train acc: 0.5749, val loss: 0.6287, val acc: 0.5703\n",
      "Epoch 1480/5000, train loss: 0.6748, train acc: 0.5794, val loss: 0.6141, val acc: 0.5938\n",
      "Epoch 1490/5000, train loss: 0.6747, train acc: 0.5757, val loss: 0.6176, val acc: 0.5984\n",
      "Epoch 1500/5000, train loss: 0.6728, train acc: 0.5789, val loss: 0.6221, val acc: 0.5594\n",
      "Epoch 1510/5000, train loss: 0.6719, train acc: 0.5888, val loss: 0.6253, val acc: 0.5656\n",
      "Epoch 1520/5000, train loss: 0.6715, train acc: 0.5925, val loss: 0.6192, val acc: 0.5844\n",
      "Epoch 1530/5000, train loss: 0.6702, train acc: 0.5925, val loss: 0.6230, val acc: 0.5891\n",
      "Epoch 1540/5000, train loss: 0.6713, train acc: 0.5864, val loss: 0.6215, val acc: 0.5875\n",
      "Epoch 1550/5000, train loss: 0.6768, train acc: 0.5820, val loss: 0.6228, val acc: 0.5750\n",
      "Epoch 1560/5000, train loss: 0.6782, train acc: 0.5748, val loss: 0.6244, val acc: 0.5594\n",
      "Epoch 1570/5000, train loss: 0.6770, train acc: 0.5792, val loss: 0.6219, val acc: 0.5766\n",
      "Epoch 1580/5000, train loss: 0.6770, train acc: 0.5786, val loss: 0.6214, val acc: 0.5969\n",
      "Epoch 1590/5000, train loss: 0.6763, train acc: 0.5792, val loss: 0.6156, val acc: 0.5875\n",
      "Epoch 1600/5000, train loss: 0.6752, train acc: 0.5741, val loss: 0.6160, val acc: 0.5938\n",
      "Epoch 1610/5000, train loss: 0.6735, train acc: 0.5866, val loss: 0.6217, val acc: 0.5750\n",
      "Epoch 1620/5000, train loss: 0.6754, train acc: 0.5724, val loss: 0.6215, val acc: 0.5891\n",
      "Epoch 1630/5000, train loss: 0.6751, train acc: 0.5692, val loss: 0.6215, val acc: 0.5938\n",
      "Epoch 1640/5000, train loss: 0.6765, train acc: 0.5678, val loss: 0.6103, val acc: 0.6094\n",
      "Epoch 1650/5000, train loss: 0.6755, train acc: 0.5612, val loss: 0.6272, val acc: 0.5406\n",
      "Epoch 1660/5000, train loss: 0.6745, train acc: 0.5716, val loss: 0.6206, val acc: 0.6078\n",
      "Epoch 1670/5000, train loss: 0.6718, train acc: 0.5612, val loss: 0.6239, val acc: 0.5672\n",
      "Epoch 1680/5000, train loss: 0.6723, train acc: 0.5778, val loss: 0.6234, val acc: 0.5859\n",
      "Epoch 1690/5000, train loss: 0.6696, train acc: 0.5863, val loss: 0.6183, val acc: 0.5828\n",
      "Epoch 1700/5000, train loss: 0.6673, train acc: 0.5920, val loss: 0.6168, val acc: 0.6062\n",
      "Epoch 1710/5000, train loss: 0.6676, train acc: 0.5931, val loss: 0.6187, val acc: 0.5922\n",
      "Epoch 1720/5000, train loss: 0.6714, train acc: 0.5871, val loss: 0.6223, val acc: 0.5891\n",
      "Epoch 1730/5000, train loss: 0.6694, train acc: 0.5922, val loss: 0.6163, val acc: 0.5906\n",
      "Epoch 1740/5000, train loss: 0.6675, train acc: 0.5958, val loss: 0.6157, val acc: 0.6078\n",
      "Epoch 1750/5000, train loss: 0.6682, train acc: 0.5958, val loss: 0.6150, val acc: 0.5875\n",
      "Epoch 1760/5000, train loss: 0.6687, train acc: 0.5888, val loss: 0.6195, val acc: 0.6016\n",
      "Epoch 1770/5000, train loss: 0.6719, train acc: 0.5832, val loss: 0.6177, val acc: 0.6094\n",
      "Epoch 1780/5000, train loss: 0.6726, train acc: 0.5866, val loss: 0.6170, val acc: 0.6031\n",
      "Epoch 1790/5000, train loss: 0.6692, train acc: 0.5934, val loss: 0.6143, val acc: 0.6062\n",
      "Epoch 1800/5000, train loss: 0.6706, train acc: 0.5907, val loss: 0.6188, val acc: 0.5984\n",
      "Epoch 1810/5000, train loss: 0.6706, train acc: 0.5909, val loss: 0.6167, val acc: 0.5906\n",
      "Epoch 1820/5000, train loss: 0.6688, train acc: 0.5944, val loss: 0.6152, val acc: 0.5844\n",
      "Epoch 1830/5000, train loss: 0.6730, train acc: 0.5894, val loss: 0.6227, val acc: 0.5719\n",
      "Epoch 1840/5000, train loss: 0.6751, train acc: 0.5829, val loss: 0.6207, val acc: 0.5719\n",
      "Epoch 1850/5000, train loss: 0.6771, train acc: 0.5740, val loss: 0.6247, val acc: 0.5453\n",
      "Epoch 1860/5000, train loss: 0.6770, train acc: 0.5673, val loss: 0.6265, val acc: 0.5687\n",
      "Epoch 1870/5000, train loss: 0.6781, train acc: 0.5703, val loss: 0.6220, val acc: 0.5891\n",
      "Epoch 1880/5000, train loss: 0.6767, train acc: 0.5815, val loss: 0.6269, val acc: 0.5594\n",
      "Epoch 1890/5000, train loss: 0.6770, train acc: 0.5757, val loss: 0.6242, val acc: 0.5781\n",
      "Epoch 1900/5000, train loss: 0.6735, train acc: 0.5764, val loss: 0.6197, val acc: 0.5687\n",
      "Epoch 1910/5000, train loss: 0.6759, train acc: 0.5719, val loss: 0.6195, val acc: 0.5609\n",
      "Epoch 1920/5000, train loss: 0.6719, train acc: 0.5756, val loss: 0.6195, val acc: 0.5656\n",
      "Epoch 1930/5000, train loss: 0.6757, train acc: 0.5834, val loss: 0.6235, val acc: 0.5687\n",
      "Epoch 1940/5000, train loss: 0.6744, train acc: 0.5848, val loss: 0.6339, val acc: 0.5484\n",
      "Epoch 1950/5000, train loss: 0.6737, train acc: 0.5843, val loss: 0.6267, val acc: 0.5625\n",
      "Epoch 1960/5000, train loss: 0.6717, train acc: 0.5885, val loss: 0.6243, val acc: 0.5750\n",
      "Epoch 1970/5000, train loss: 0.6686, train acc: 0.5968, val loss: 0.6233, val acc: 0.5813\n",
      "Epoch 1980/5000, train loss: 0.6691, train acc: 0.5993, val loss: 0.6160, val acc: 0.5875\n",
      "Epoch 1990/5000, train loss: 0.6649, train acc: 0.6016, val loss: 0.6108, val acc: 0.6156\n",
      "Epoch 2000/5000, train loss: 0.6683, train acc: 0.6046, val loss: 0.6140, val acc: 0.5891\n",
      "Epoch 2010/5000, train loss: 0.6652, train acc: 0.6071, val loss: 0.6114, val acc: 0.6125\n",
      "Epoch 2020/5000, train loss: 0.6677, train acc: 0.5998, val loss: 0.6124, val acc: 0.5953\n",
      "Epoch 2030/5000, train loss: 0.6685, train acc: 0.5958, val loss: 0.6139, val acc: 0.5891\n",
      "Epoch 2040/5000, train loss: 0.6694, train acc: 0.5933, val loss: 0.6144, val acc: 0.5891\n",
      "Epoch 2050/5000, train loss: 0.6716, train acc: 0.5863, val loss: 0.6201, val acc: 0.5828\n",
      "Epoch 2060/5000, train loss: 0.6752, train acc: 0.5791, val loss: 0.6208, val acc: 0.5813\n",
      "Epoch 2070/5000, train loss: 0.6710, train acc: 0.5882, val loss: 0.6118, val acc: 0.5875\n",
      "Epoch 2080/5000, train loss: 0.6714, train acc: 0.5810, val loss: 0.6191, val acc: 0.5953\n",
      "Epoch 2090/5000, train loss: 0.6721, train acc: 0.5850, val loss: 0.6178, val acc: 0.5906\n",
      "Epoch 2100/5000, train loss: 0.6704, train acc: 0.5879, val loss: 0.6181, val acc: 0.6016\n",
      "Epoch 2110/5000, train loss: 0.6697, train acc: 0.5899, val loss: 0.6194, val acc: 0.5859\n",
      "Epoch 2120/5000, train loss: 0.6731, train acc: 0.5829, val loss: 0.6197, val acc: 0.5891\n",
      "Epoch 2130/5000, train loss: 0.6697, train acc: 0.5930, val loss: 0.6234, val acc: 0.5750\n",
      "Epoch 2140/5000, train loss: 0.6728, train acc: 0.5834, val loss: 0.6144, val acc: 0.6172\n",
      "Epoch 2150/5000, train loss: 0.6710, train acc: 0.5894, val loss: 0.6130, val acc: 0.5891\n",
      "Epoch 2160/5000, train loss: 0.6706, train acc: 0.5875, val loss: 0.6152, val acc: 0.5859\n",
      "Epoch 2170/5000, train loss: 0.6725, train acc: 0.5848, val loss: 0.6159, val acc: 0.5719\n",
      "Epoch 2180/5000, train loss: 0.6745, train acc: 0.5791, val loss: 0.6206, val acc: 0.5734\n",
      "Epoch 2190/5000, train loss: 0.6722, train acc: 0.5808, val loss: 0.6175, val acc: 0.5891\n",
      "Epoch 2200/5000, train loss: 0.6740, train acc: 0.5909, val loss: 0.6216, val acc: 0.5687\n",
      "Epoch 2210/5000, train loss: 0.6726, train acc: 0.5837, val loss: 0.6165, val acc: 0.5734\n",
      "Epoch 2220/5000, train loss: 0.6677, train acc: 0.6004, val loss: 0.6166, val acc: 0.6000\n",
      "Epoch 2230/5000, train loss: 0.6728, train acc: 0.5891, val loss: 0.6106, val acc: 0.6156\n",
      "Epoch 2240/5000, train loss: 0.6697, train acc: 0.6000, val loss: 0.6204, val acc: 0.5781\n",
      "Epoch 2250/5000, train loss: 0.6709, train acc: 0.5920, val loss: 0.6254, val acc: 0.5641\n",
      "Epoch 2260/5000, train loss: 0.6722, train acc: 0.5949, val loss: 0.6186, val acc: 0.5813\n",
      "Epoch 2270/5000, train loss: 0.6670, train acc: 0.5979, val loss: 0.6238, val acc: 0.5594\n",
      "Epoch 2280/5000, train loss: 0.6712, train acc: 0.5917, val loss: 0.6175, val acc: 0.6031\n",
      "Epoch 2290/5000, train loss: 0.6676, train acc: 0.5971, val loss: 0.6231, val acc: 0.5828\n",
      "Epoch 2300/5000, train loss: 0.6662, train acc: 0.5971, val loss: 0.6177, val acc: 0.5687\n",
      "Epoch 2310/5000, train loss: 0.6706, train acc: 0.5904, val loss: 0.6211, val acc: 0.5938\n",
      "Epoch 2320/5000, train loss: 0.6720, train acc: 0.5864, val loss: 0.6189, val acc: 0.5922\n",
      "Epoch 2330/5000, train loss: 0.6731, train acc: 0.5804, val loss: 0.6194, val acc: 0.5906\n",
      "Epoch 2340/5000, train loss: 0.6695, train acc: 0.5953, val loss: 0.6214, val acc: 0.5922\n",
      "Epoch 2350/5000, train loss: 0.6713, train acc: 0.5894, val loss: 0.6152, val acc: 0.6062\n",
      "Epoch 2360/5000, train loss: 0.6724, train acc: 0.5850, val loss: 0.6158, val acc: 0.5859\n",
      "Epoch 2370/5000, train loss: 0.6728, train acc: 0.5856, val loss: 0.6236, val acc: 0.5672\n",
      "Epoch 2380/5000, train loss: 0.6736, train acc: 0.5899, val loss: 0.6217, val acc: 0.5703\n",
      "Epoch 2390/5000, train loss: 0.6768, train acc: 0.5807, val loss: 0.6244, val acc: 0.5625\n",
      "Epoch 2400/5000, train loss: 0.6733, train acc: 0.5872, val loss: 0.6154, val acc: 0.6016\n",
      "Epoch 2410/5000, train loss: 0.6706, train acc: 0.5882, val loss: 0.6144, val acc: 0.5875\n",
      "Epoch 2420/5000, train loss: 0.6721, train acc: 0.5928, val loss: 0.6152, val acc: 0.5844\n",
      "Epoch 2430/5000, train loss: 0.6698, train acc: 0.5882, val loss: 0.6161, val acc: 0.5844\n",
      "Epoch 2440/5000, train loss: 0.6694, train acc: 0.5996, val loss: 0.6166, val acc: 0.5922\n",
      "Epoch 2450/5000, train loss: 0.6649, train acc: 0.5952, val loss: 0.6263, val acc: 0.5813\n",
      "Epoch 2460/5000, train loss: 0.6716, train acc: 0.5869, val loss: 0.6230, val acc: 0.5813\n",
      "Epoch 2470/5000, train loss: 0.6719, train acc: 0.5816, val loss: 0.6173, val acc: 0.5844\n",
      "Epoch 2480/5000, train loss: 0.6701, train acc: 0.5880, val loss: 0.6203, val acc: 0.5875\n",
      "Epoch 2490/5000, train loss: 0.6711, train acc: 0.5834, val loss: 0.6222, val acc: 0.5953\n",
      "Epoch 2500/5000, train loss: 0.6712, train acc: 0.5885, val loss: 0.6127, val acc: 0.5969\n",
      "Epoch 2510/5000, train loss: 0.6703, train acc: 0.5888, val loss: 0.6174, val acc: 0.6125\n",
      "Epoch 2520/5000, train loss: 0.6672, train acc: 0.5936, val loss: 0.6172, val acc: 0.6125\n",
      "Epoch 2530/5000, train loss: 0.6699, train acc: 0.5933, val loss: 0.6122, val acc: 0.6031\n",
      "Epoch 2540/5000, train loss: 0.6706, train acc: 0.5922, val loss: 0.6137, val acc: 0.5875\n",
      "Epoch 2550/5000, train loss: 0.6700, train acc: 0.5885, val loss: 0.6154, val acc: 0.5781\n",
      "Epoch 2560/5000, train loss: 0.6702, train acc: 0.5886, val loss: 0.6151, val acc: 0.5844\n",
      "Epoch 2570/5000, train loss: 0.6681, train acc: 0.5915, val loss: 0.6127, val acc: 0.5844\n",
      "Epoch 2580/5000, train loss: 0.6693, train acc: 0.5909, val loss: 0.6136, val acc: 0.5859\n",
      "Epoch 2590/5000, train loss: 0.6702, train acc: 0.5953, val loss: 0.6176, val acc: 0.6031\n",
      "Epoch 2600/5000, train loss: 0.6668, train acc: 0.5965, val loss: 0.6186, val acc: 0.5938\n",
      "Epoch 2610/5000, train loss: 0.6672, train acc: 0.5960, val loss: 0.6180, val acc: 0.5719\n",
      "Epoch 2620/5000, train loss: 0.6679, train acc: 0.5976, val loss: 0.6126, val acc: 0.5906\n",
      "Epoch 2630/5000, train loss: 0.6665, train acc: 0.6105, val loss: 0.6117, val acc: 0.6031\n",
      "Epoch 2640/5000, train loss: 0.6644, train acc: 0.6073, val loss: 0.6046, val acc: 0.6141\n",
      "Epoch 2650/5000, train loss: 0.6663, train acc: 0.6054, val loss: 0.6202, val acc: 0.5906\n",
      "Epoch 2660/5000, train loss: 0.6643, train acc: 0.6052, val loss: 0.6165, val acc: 0.6078\n",
      "Epoch 2670/5000, train loss: 0.6682, train acc: 0.6012, val loss: 0.6153, val acc: 0.6047\n",
      "Epoch 2680/5000, train loss: 0.6678, train acc: 0.5973, val loss: 0.6127, val acc: 0.6094\n",
      "Epoch 2690/5000, train loss: 0.6675, train acc: 0.6017, val loss: 0.6086, val acc: 0.6109\n",
      "Epoch 2700/5000, train loss: 0.6709, train acc: 0.5974, val loss: 0.6149, val acc: 0.6031\n",
      "Epoch 2710/5000, train loss: 0.6700, train acc: 0.5963, val loss: 0.6134, val acc: 0.6047\n",
      "Epoch 2720/5000, train loss: 0.6725, train acc: 0.5915, val loss: 0.6174, val acc: 0.5813\n",
      "Epoch 2730/5000, train loss: 0.6719, train acc: 0.5918, val loss: 0.6160, val acc: 0.5859\n",
      "Epoch 2740/5000, train loss: 0.6690, train acc: 0.5953, val loss: 0.6126, val acc: 0.5953\n",
      "Epoch 2750/5000, train loss: 0.6727, train acc: 0.5856, val loss: 0.6212, val acc: 0.5813\n",
      "Epoch 2760/5000, train loss: 0.6709, train acc: 0.5977, val loss: 0.6165, val acc: 0.5953\n",
      "Epoch 2770/5000, train loss: 0.6723, train acc: 0.5893, val loss: 0.6205, val acc: 0.6000\n",
      "Epoch 2780/5000, train loss: 0.6719, train acc: 0.5883, val loss: 0.6208, val acc: 0.6000\n",
      "Epoch 2790/5000, train loss: 0.6715, train acc: 0.5842, val loss: 0.6127, val acc: 0.5984\n",
      "Epoch 2800/5000, train loss: 0.6693, train acc: 0.5987, val loss: 0.6183, val acc: 0.5797\n",
      "Epoch 2810/5000, train loss: 0.6716, train acc: 0.5979, val loss: 0.6189, val acc: 0.6016\n",
      "Epoch 2820/5000, train loss: 0.6649, train acc: 0.6113, val loss: 0.6148, val acc: 0.6000\n",
      "Epoch 2830/5000, train loss: 0.6671, train acc: 0.6067, val loss: 0.6157, val acc: 0.6047\n",
      "Epoch 2840/5000, train loss: 0.6723, train acc: 0.6028, val loss: 0.6149, val acc: 0.6000\n",
      "Epoch 2850/5000, train loss: 0.6684, train acc: 0.6027, val loss: 0.6203, val acc: 0.5875\n",
      "Epoch 2860/5000, train loss: 0.6688, train acc: 0.5979, val loss: 0.6113, val acc: 0.6078\n",
      "Epoch 2870/5000, train loss: 0.6699, train acc: 0.6025, val loss: 0.6198, val acc: 0.5859\n",
      "Epoch 2880/5000, train loss: 0.6712, train acc: 0.5961, val loss: 0.6170, val acc: 0.5859\n",
      "Epoch 2890/5000, train loss: 0.6730, train acc: 0.5842, val loss: 0.6183, val acc: 0.5766\n",
      "Epoch 2900/5000, train loss: 0.6708, train acc: 0.5925, val loss: 0.6184, val acc: 0.5687\n",
      "Epoch 2910/5000, train loss: 0.6721, train acc: 0.5907, val loss: 0.6165, val acc: 0.5828\n",
      "Epoch 2920/5000, train loss: 0.6702, train acc: 0.5907, val loss: 0.6171, val acc: 0.5938\n",
      "Epoch 2930/5000, train loss: 0.6710, train acc: 0.5879, val loss: 0.6177, val acc: 0.5750\n",
      "Epoch 2940/5000, train loss: 0.6727, train acc: 0.5901, val loss: 0.6180, val acc: 0.5906\n",
      "Epoch 2950/5000, train loss: 0.6735, train acc: 0.5981, val loss: 0.6248, val acc: 0.5766\n",
      "Epoch 2960/5000, train loss: 0.6714, train acc: 0.5938, val loss: 0.6206, val acc: 0.6047\n",
      "Epoch 2970/5000, train loss: 0.6703, train acc: 0.5969, val loss: 0.6205, val acc: 0.5938\n",
      "Epoch 2980/5000, train loss: 0.6731, train acc: 0.5880, val loss: 0.6254, val acc: 0.5813\n",
      "Epoch 2990/5000, train loss: 0.6683, train acc: 0.5957, val loss: 0.6186, val acc: 0.5984\n",
      "Epoch 3000/5000, train loss: 0.6708, train acc: 0.5840, val loss: 0.6144, val acc: 0.6109\n",
      "Epoch 3010/5000, train loss: 0.6671, train acc: 0.5807, val loss: 0.6213, val acc: 0.5844\n",
      "Epoch 3020/5000, train loss: 0.6671, train acc: 0.5896, val loss: 0.6206, val acc: 0.6109\n",
      "Epoch 3030/5000, train loss: 0.6682, train acc: 0.5930, val loss: 0.6113, val acc: 0.5969\n",
      "Epoch 3040/5000, train loss: 0.6732, train acc: 0.5789, val loss: 0.6253, val acc: 0.5719\n",
      "Epoch 3050/5000, train loss: 0.6715, train acc: 0.5882, val loss: 0.6179, val acc: 0.5719\n",
      "Epoch 3060/5000, train loss: 0.6701, train acc: 0.5969, val loss: 0.6158, val acc: 0.5922\n",
      "Epoch 3070/5000, train loss: 0.6709, train acc: 0.5966, val loss: 0.6178, val acc: 0.5969\n",
      "Epoch 3080/5000, train loss: 0.6679, train acc: 0.5976, val loss: 0.6204, val acc: 0.5969\n",
      "Epoch 3090/5000, train loss: 0.6691, train acc: 0.5977, val loss: 0.6124, val acc: 0.5984\n",
      "Epoch 3100/5000, train loss: 0.6701, train acc: 0.5888, val loss: 0.6123, val acc: 0.6094\n",
      "Epoch 3110/5000, train loss: 0.6707, train acc: 0.5799, val loss: 0.6239, val acc: 0.5797\n",
      "Epoch 3120/5000, train loss: 0.6717, train acc: 0.5804, val loss: 0.6189, val acc: 0.5844\n",
      "Epoch 3130/5000, train loss: 0.6688, train acc: 0.5827, val loss: 0.6194, val acc: 0.5734\n",
      "Epoch 3140/5000, train loss: 0.6718, train acc: 0.5791, val loss: 0.6153, val acc: 0.6047\n",
      "Epoch 3150/5000, train loss: 0.6710, train acc: 0.5904, val loss: 0.6172, val acc: 0.5922\n",
      "Epoch 3160/5000, train loss: 0.6702, train acc: 0.5773, val loss: 0.6131, val acc: 0.5859\n",
      "Epoch 3170/5000, train loss: 0.6745, train acc: 0.5786, val loss: 0.6285, val acc: 0.5766\n",
      "Epoch 3180/5000, train loss: 0.6727, train acc: 0.5930, val loss: 0.6251, val acc: 0.5734\n",
      "Epoch 3190/5000, train loss: 0.6734, train acc: 0.5850, val loss: 0.6166, val acc: 0.6000\n",
      "Epoch 3200/5000, train loss: 0.6721, train acc: 0.5847, val loss: 0.6217, val acc: 0.5813\n",
      "Epoch 3210/5000, train loss: 0.6708, train acc: 0.5941, val loss: 0.6212, val acc: 0.5938\n",
      "Epoch 3220/5000, train loss: 0.6726, train acc: 0.5938, val loss: 0.6201, val acc: 0.5875\n",
      "Epoch 3230/5000, train loss: 0.6745, train acc: 0.5866, val loss: 0.6205, val acc: 0.5687\n",
      "Epoch 3240/5000, train loss: 0.6740, train acc: 0.5824, val loss: 0.6225, val acc: 0.5828\n",
      "Epoch 3250/5000, train loss: 0.6743, train acc: 0.5807, val loss: 0.6233, val acc: 0.5781\n",
      "Epoch 3260/5000, train loss: 0.6730, train acc: 0.5869, val loss: 0.6218, val acc: 0.5813\n",
      "Epoch 3270/5000, train loss: 0.6715, train acc: 0.5875, val loss: 0.6207, val acc: 0.5969\n",
      "Epoch 3280/5000, train loss: 0.6725, train acc: 0.5879, val loss: 0.6201, val acc: 0.5813\n",
      "Epoch 3290/5000, train loss: 0.6694, train acc: 0.5952, val loss: 0.6231, val acc: 0.5922\n",
      "Epoch 3300/5000, train loss: 0.6693, train acc: 0.5907, val loss: 0.6163, val acc: 0.5828\n",
      "Epoch 3310/5000, train loss: 0.6683, train acc: 0.5952, val loss: 0.6206, val acc: 0.6047\n",
      "Epoch 3320/5000, train loss: 0.6688, train acc: 0.5969, val loss: 0.6226, val acc: 0.6016\n",
      "Epoch 3330/5000, train loss: 0.6687, train acc: 0.5926, val loss: 0.6231, val acc: 0.5906\n",
      "Epoch 3340/5000, train loss: 0.6717, train acc: 0.5899, val loss: 0.6252, val acc: 0.5578\n",
      "Epoch 3350/5000, train loss: 0.6702, train acc: 0.5936, val loss: 0.6156, val acc: 0.5953\n",
      "Epoch 3360/5000, train loss: 0.6685, train acc: 0.5904, val loss: 0.6226, val acc: 0.5781\n",
      "Epoch 3370/5000, train loss: 0.6703, train acc: 0.5909, val loss: 0.6169, val acc: 0.6234\n",
      "Epoch 3380/5000, train loss: 0.6690, train acc: 0.5971, val loss: 0.6119, val acc: 0.6141\n",
      "Epoch 3390/5000, train loss: 0.6713, train acc: 0.5950, val loss: 0.6144, val acc: 0.6109\n",
      "Epoch 3400/5000, train loss: 0.6697, train acc: 0.5957, val loss: 0.6195, val acc: 0.5922\n",
      "Epoch 3410/5000, train loss: 0.6665, train acc: 0.5957, val loss: 0.6176, val acc: 0.6094\n",
      "Epoch 3420/5000, train loss: 0.6635, train acc: 0.6111, val loss: 0.6197, val acc: 0.5813\n",
      "Epoch 3430/5000, train loss: 0.6653, train acc: 0.6105, val loss: 0.6127, val acc: 0.6000\n",
      "Epoch 3440/5000, train loss: 0.6647, train acc: 0.6059, val loss: 0.6168, val acc: 0.6016\n",
      "Epoch 3450/5000, train loss: 0.6672, train acc: 0.5968, val loss: 0.6142, val acc: 0.6016\n",
      "Epoch 3460/5000, train loss: 0.6657, train acc: 0.6046, val loss: 0.6206, val acc: 0.5813\n",
      "Epoch 3470/5000, train loss: 0.6638, train acc: 0.6044, val loss: 0.6164, val acc: 0.5813\n",
      "Epoch 3480/5000, train loss: 0.6668, train acc: 0.6055, val loss: 0.6202, val acc: 0.5797\n",
      "Epoch 3490/5000, train loss: 0.6655, train acc: 0.6033, val loss: 0.6156, val acc: 0.6031\n",
      "Epoch 3500/5000, train loss: 0.6669, train acc: 0.5958, val loss: 0.6158, val acc: 0.5734\n",
      "Epoch 3510/5000, train loss: 0.6654, train acc: 0.6022, val loss: 0.6133, val acc: 0.6031\n",
      "Epoch 3520/5000, train loss: 0.6674, train acc: 0.5981, val loss: 0.6163, val acc: 0.5875\n",
      "Epoch 3530/5000, train loss: 0.6676, train acc: 0.6004, val loss: 0.6222, val acc: 0.5766\n",
      "Epoch 3540/5000, train loss: 0.6659, train acc: 0.5907, val loss: 0.6193, val acc: 0.5938\n",
      "Epoch 3550/5000, train loss: 0.6644, train acc: 0.6063, val loss: 0.6231, val acc: 0.5766\n",
      "Epoch 3560/5000, train loss: 0.6655, train acc: 0.6004, val loss: 0.6170, val acc: 0.5953\n",
      "Epoch 3570/5000, train loss: 0.6667, train acc: 0.6000, val loss: 0.6190, val acc: 0.5859\n",
      "Epoch 3580/5000, train loss: 0.6679, train acc: 0.6062, val loss: 0.6198, val acc: 0.5641\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9652/3873615880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# best_model, history = train_model(dataloaders, dataset_sizes, model, criterion, optimizer, scheduler, PARAMS['N_EPOCHS'], random_seed=random_seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/thesis-2022/train.py\u001b[0m in \u001b[0;36mtrain_model_2\u001b[0;34m(model, optimizer, loss_fn, train_dl, val_dl, epochs, random_seed, device)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m         \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mnum_train_correct\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mnum_train_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GCNAuto(in_features=SEQ_LEN, \n",
    "        n_nodes=PARAMS['N_CHANNELS'], \n",
    "        num_classes=PARAMS['N_CLASSES'], \n",
    "        hidden_sizes=PARAMS['GCNAUTO_HIDDEN_SIZES'], \n",
    "        dropout_p=0.4, \n",
    "        device=PARAMS['DEVICE'])\n",
    "\n",
    "model = GCRAM(graph_type='n', \n",
    "        seq_len=SEQ_LEN, \n",
    "        cnn_in_channels=PARAMS['GCRAM_CNN_IN_CHANNELS'], \n",
    "        cnn_n_kernels=PARAMS['GCRAM_CNN_N_KERNELS'], \n",
    "        cnn_kernel_size=PARAMS['GCRAM_CNN_KERNEL_SIZE'], \n",
    "        cnn_stride=PARAMS['GCRAM_CNN_STRIDE'], \n",
    "        maxpool_kernel_size=PARAMS['GCRAM_MAXPOOL_KERNEL_SIZE'], \n",
    "        maxpool_stride=PARAMS['GCRAM_MAXPOOL_STRIDE'], \n",
    "        lstm_hidden_size=PARAMS['GCRAM_LSTM_HIDDEN_SIZE'], \n",
    "        is_bidirectional=PARAMS['GCRAM_LSTM_IS_BIDIRECTIONAL'], \n",
    "        lstm_n_layers=PARAMS['GCRAM_LSTM_N_LAYERS'], \n",
    "        attn_embed_dim=PARAMS['GCRAM_ATTN_EMBED_DIM'], \n",
    "        n_classes=PARAMS['N_CLASSES'], \n",
    "        lstm_dropout_p=PARAMS['GCRAM_LSTM_DROPOUT_P'], \n",
    "        dropout1_p=PARAMS['GCRAM_DROPOUT1_P'], \n",
    "        dropout2_p=PARAMS['GCRAM_DROPOUT2_P'], \n",
    "        device=PARAMS['DEVICE'])\n",
    "\n",
    "model = init_model_params(model, random_seed=RANDOM_SEED)\n",
    "# model.init_node_embeddings()\n",
    "\n",
    "model = model.to(PARAMS['DEVICE'])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=PARAMS['LR'])\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, PARAMS['SCHEDULER_STEPSIZE'], PARAMS['SCHEDULER_GAMMA'])\n",
    "\n",
    "# best_model, history = train_model(dataloaders, dataset_sizes, model, criterion, optimizer, scheduler, PARAMS['N_EPOCHS'], random_seed=random_seed)\n",
    "best_model, history = train_model_2(model, optimizer, criterion, dataloaders['train'], dataloaders['val'], N_EPOCHS, RANDOM_SEED, PARAMS['DEVICE'])\n",
    "best_model = best_model.to(PARAMS['DEVICE'])\n",
    "\n",
    "y_preds, y_test = model_predict(best_model, test_loader=dataloaders['test'])\n",
    "\n",
    "cr, cm, auroc = print_classification_report(y_test, y_preds, PARAMS['N_CLASSES'], PARAMS['LABEL_MAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.5942028985507246, 'recall': 0.643979057591623, 'f1-score': 0.6180904522613065, 'support': 382}, '1': {'precision': 0.615819209039548, 'recall': 0.5647668393782384, 'f1-score': 0.5891891891891892, 'support': 386}, 'accuracy': 0.6041666666666666, 'macro avg': {'precision': 0.6050110537951363, 'recall': 0.6043729484849307, 'f1-score': 0.6036398207252478, 'support': 768}, 'weighted avg': {'precision': 0.6050673462703676, 'recall': 0.6041666666666666, 'f1-score': 0.6035645570193309, 'support': 768}}\n"
     ]
    }
   ],
   "source": [
    "y_preds, y_test = model_predict(model, test_loader=dataloaders['test'])\n",
    "\n",
    "cr, cm, auroc = print_classification_report(y_test, y_preds, PARAMS['N_CLASSES'], PARAMS['LABEL_MAP'])\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84c85347d78e9e1e10700c6d8e0f5eee9b662a9651d925575cf1524e055e4541"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
