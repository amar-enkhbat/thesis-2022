{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import prepare_data\n",
    "from train import get_dataloaders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from params import PARAMS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from main import init_model_params\n",
    "\n",
    "from train import train_model_2\n",
    "from main import model_predict, print_classification_report\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "SEQ_LEN = 32\n",
    "N_EPOCHS = 500\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "X_train shape: (293612, 64)\n",
      "y_train shape: (293612,)\n",
      "X_test shape: (59040, 64)\n",
      "y_test shape: (59040,)\n",
      "Shape with 2 classes (left, right arm):\n",
      "X_train shape: (146060, 64)\n",
      "y_train shape: (146060,)\n",
      "X_test shape: (29520, 64)\n",
      "y_test shape: (29520,)\n"
     ]
    }
   ],
   "source": [
    "from utils import load_data\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data('dataset/train/cross_subject_data_0_5_subjects.pickle')\n",
    "print('Original shapes:')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "X_train = X_train[np.isin(y_train, [0, 1])]\n",
    "y_train = y_train[np.isin(y_train, [0, 1])]\n",
    "X_test = X_test[np.isin(y_test, [0, 1])]\n",
    "y_test = y_test[np.isin(y_test, [0, 1])]\n",
    "\n",
    "print('Shape with 2 classes (left, right arm):')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "X_train = np.vstack([X_train, X_test])\n",
    "y_train = np.hstack([y_train, y_test])\n",
    "\n",
    "X_train, y_train = prepare_data(X_train, y_train, SEQ_LEN)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=PARAMS['TEST_SIZE'], shuffle=True, random_state=RANDOM_SEED)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=PARAMS['VALID_SIZE'], shuffle=True, random_state=RANDOM_SEED)\n",
    "dataloaders = get_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test, PARAMS['BATCH_SIZE'], random_seed=RANDOM_SEED, device=PARAMS['DEVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import SelfAttentionLayer, BatchGraphConvolutionLayer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class GCRAMAuto(nn.Module):\n",
    "    def __init__(self, seq_len, cnn_in_channels, cnn_n_kernels, cnn_kernel_size, cnn_stride, maxpool_kernel_size, maxpool_stride, lstm_hidden_size, is_bidirectional, lstm_n_layers, attn_embed_dim, n_classes, lstm_dropout_p, dropout1_p, dropout2_p, device):\n",
    "        super(GCRAMAuto, self).__init__()\n",
    "\n",
    "        self.dropout1_p = dropout1_p\n",
    "        self.dropout2_p = dropout2_p\n",
    "\n",
    "        self.gc1 = BatchGraphConvolutionLayer(seq_len, 1024, 64)\n",
    "        self.gc2 = BatchGraphConvolutionLayer(1024, seq_len, 64)\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(cnn_in_channels, cnn_n_kernels, kernel_size=cnn_kernel_size, stride=cnn_stride)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=maxpool_stride)\n",
    "\n",
    "        cnn_output_size = (seq_len - cnn_kernel_size[1])//cnn_stride + 1\n",
    "        maxpool_output_size = (cnn_output_size-maxpool_kernel_size[1])//maxpool_stride+1\n",
    "        lstm_input_size = maxpool_output_size * cnn_in_channels * cnn_n_kernels\n",
    "        self.lstm1 = nn.LSTM(input_size=lstm_input_size, hidden_size=lstm_hidden_size, batch_first=True, bidirectional=is_bidirectional, num_layers=lstm_n_layers, dropout=lstm_dropout_p)\n",
    "\n",
    "        if is_bidirectional:\n",
    "            self.attention = SelfAttentionLayer(hidden_size=lstm_hidden_size*2, attention_size=attn_embed_dim, return_alphas=True)\n",
    "        else:\n",
    "            self.attention = SelfAttentionLayer(hidden_size=lstm_hidden_size, attention_size=attn_embed_dim, return_alphas=True)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        if is_bidirectional:\n",
    "            self.linear = nn.Linear(lstm_hidden_size*2, n_classes)\n",
    "        else:\n",
    "            self.linear = nn.Linear(lstm_hidden_size, n_classes)\n",
    "\n",
    "        self.adj = nn.Parameter(torch.randn(64, 64), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        # out = torch.einsum(\"ij,kjl->kil\", self.adj, x)\\\n",
    "        out = self.gc1(x, self.adj)\n",
    "        # print(out.size())\n",
    "        out = self.gc2(out, self.adj)\n",
    "        # print(out.size())\n",
    "\n",
    "        out = out.unsqueeze(1)\n",
    "\n",
    "        out = F.relu(self.conv1(out))\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        out = self.flatten(out)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = F.dropout(out, p=self.dropout1_p)\n",
    "\n",
    "        out, (h_T, c_T) = self.lstm1(out)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        out = out.unsqueeze(1)\n",
    "\n",
    "        out, attn_weights = self.attention(out)\n",
    "        out = F.dropout(out, p=self.dropout2_p)\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_node_embeddings(self):\n",
    "        stdv = 1. / math.sqrt(self.adj.size(1))\n",
    "        self.adj.data.uniform_(-stdv, stdv)\n",
    "        self.adj.data.fill_diagonal_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -500: [256, -500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9084/2459382109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdropout1_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GCRAM_DROPOUT1_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdropout2_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GCRAM_DROPOUT2_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         device=PARAMS['DEVICE']).to(PARAMS['DEVICE'])\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9084/1057425784.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, seq_len, cnn_in_channels, cnn_n_kernels, cnn_kernel_size, cnn_stride, maxpool_kernel_size, maxpool_stride, lstm_hidden_size, is_bidirectional, lstm_n_layers, attn_embed_dim, n_classes, lstm_dropout_p, dropout1_p, dropout2_p, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmaxpool_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcnn_output_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmaxpool_kernel_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mmaxpool_stride\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlstm_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxpool_output_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcnn_in_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcnn_n_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_input_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_bidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_dropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_bidirectional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/thesis-2022/env/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_cell_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/thesis-2022/env/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size, device, dtype)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mlayer_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mreal_hidden_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mw_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0mw_hh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mb_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -500: [256, -500]"
     ]
    }
   ],
   "source": [
    "model = GCRAMAuto(seq_len=SEQ_LEN, \n",
    "        cnn_in_channels=PARAMS['GCRAM_CNN_IN_CHANNELS'], \n",
    "        cnn_n_kernels=PARAMS['GCRAM_CNN_N_KERNELS'], \n",
    "        cnn_kernel_size=PARAMS['GCRAM_CNN_KERNEL_SIZE'], \n",
    "        cnn_stride=PARAMS['GCRAM_CNN_STRIDE'], \n",
    "        maxpool_kernel_size=PARAMS['GCRAM_MAXPOOL_KERNEL_SIZE'], \n",
    "        maxpool_stride=PARAMS['GCRAM_MAXPOOL_STRIDE'], \n",
    "        lstm_hidden_size=PARAMS['GCRAM_LSTM_HIDDEN_SIZE'], \n",
    "        is_bidirectional=PARAMS['GCRAM_LSTM_IS_BIDIRECTIONAL'], \n",
    "        lstm_n_layers=PARAMS['GCRAM_LSTM_N_LAYERS'], \n",
    "        attn_embed_dim=PARAMS['GCRAM_ATTN_EMBED_DIM'], \n",
    "        n_classes=PARAMS['N_CLASSES'], \n",
    "        lstm_dropout_p=PARAMS['GCRAM_LSTM_DROPOUT_P'], \n",
    "        dropout1_p=PARAMS['GCRAM_DROPOUT1_P'], \n",
    "        dropout2_p=PARAMS['GCRAM_DROPOUT2_P'], \n",
    "        device=PARAMS['DEVICE']).to(PARAMS['DEVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "for input, label in dataloaders['train']:\n",
    "    input = input.to(PARAMS['DEVICE'])\n",
    "    label = label.to(PARAMS['DEVICE'])\n",
    "    out = model(input)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=GCRAMAuto, opt=Adam(lr=0.001000), epochs=500, device=cuda\n",
      "\n",
      "Epoch   1/500, LR 0.0010, train loss: 0.7275, train acc: 0.5021, val loss: 0.6523, val acc: 0.4891\n",
      "Epoch  10/500, LR 0.0010, train loss: 0.6951, train acc: 0.4976, val loss: 0.6380, val acc: 0.4781\n",
      "Epoch  20/500, LR 0.0010, train loss: 0.6937, train acc: 0.5120, val loss: 0.6388, val acc: 0.4766\n",
      "Epoch  30/500, LR 0.0009, train loss: 0.6939, train acc: 0.4923, val loss: 0.6406, val acc: 0.4641\n",
      "Epoch  40/500, LR 0.0009, train loss: 0.6943, train acc: 0.5048, val loss: 0.6397, val acc: 0.4719\n",
      "Epoch  50/500, LR 0.0008, train loss: 0.6931, train acc: 0.5054, val loss: 0.6366, val acc: 0.5078\n",
      "Epoch  60/500, LR 0.0008, train loss: 0.6935, train acc: 0.5081, val loss: 0.6385, val acc: 0.4750\n",
      "Epoch  70/500, LR 0.0007, train loss: 0.6937, train acc: 0.5089, val loss: 0.6378, val acc: 0.4781\n",
      "Epoch  80/500, LR 0.0007, train loss: 0.6930, train acc: 0.5077, val loss: 0.6366, val acc: 0.4984\n",
      "Epoch  90/500, LR 0.0007, train loss: 0.6931, train acc: 0.5096, val loss: 0.6385, val acc: 0.4859\n",
      "Epoch 100/500, LR 0.0007, train loss: 0.6931, train acc: 0.5091, val loss: 0.6375, val acc: 0.4766\n",
      "Epoch 110/500, LR 0.0007, train loss: 0.6929, train acc: 0.5143, val loss: 0.6379, val acc: 0.4672\n",
      "Epoch 120/500, LR 0.0006, train loss: 0.6930, train acc: 0.5094, val loss: 0.6382, val acc: 0.4750\n",
      "Epoch 130/500, LR 0.0006, train loss: 0.6931, train acc: 0.5132, val loss: 0.6389, val acc: 0.4625\n",
      "Epoch 140/500, LR 0.0005, train loss: 0.6930, train acc: 0.5128, val loss: 0.6380, val acc: 0.4703\n",
      "Epoch 150/500, LR 0.0005, train loss: 0.6929, train acc: 0.5121, val loss: 0.6383, val acc: 0.4656\n",
      "Epoch 160/500, LR 0.0005, train loss: 0.6931, train acc: 0.5118, val loss: 0.6384, val acc: 0.4703\n",
      "Epoch 170/500, LR 0.0005, train loss: 0.6930, train acc: 0.5112, val loss: 0.6382, val acc: 0.4703\n",
      "Epoch 180/500, LR 0.0004, train loss: 0.6931, train acc: 0.5124, val loss: 0.6380, val acc: 0.4641\n",
      "Epoch 190/500, LR 0.0004, train loss: 0.6929, train acc: 0.5113, val loss: 0.6388, val acc: 0.4562\n",
      "Epoch 200/500, LR 0.0003, train loss: 0.6930, train acc: 0.5120, val loss: 0.6374, val acc: 0.4813\n",
      "Epoch 210/500, LR 0.0004, train loss: 0.6930, train acc: 0.5116, val loss: 0.6381, val acc: 0.4656\n",
      "Epoch 220/500, LR 0.0004, train loss: 0.6929, train acc: 0.5118, val loss: 0.6383, val acc: 0.4641\n",
      "Epoch 230/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6377, val acc: 0.4766\n",
      "Epoch 240/500, LR 0.0003, train loss: 0.6930, train acc: 0.5115, val loss: 0.6380, val acc: 0.4672\n",
      "Epoch 250/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6379, val acc: 0.4734\n",
      "Epoch 260/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6383, val acc: 0.4625\n",
      "Epoch 270/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6380, val acc: 0.4703\n",
      "Epoch 280/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6376, val acc: 0.4797\n",
      "Epoch 290/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6382, val acc: 0.4672\n",
      "Epoch 300/500, LR 0.0003, train loss: 0.6929, train acc: 0.5118, val loss: 0.6378, val acc: 0.4766\n",
      "Epoch 310/500, LR 0.0003, train loss: 0.6928, train acc: 0.5118, val loss: 0.6376, val acc: 0.4797\n",
      "Epoch 320/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6379, val acc: 0.4734\n",
      "Epoch 330/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6381, val acc: 0.4688\n",
      "Epoch 340/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6381, val acc: 0.4688\n",
      "Epoch 350/500, LR 0.0002, train loss: 0.6928, train acc: 0.5118, val loss: 0.6375, val acc: 0.4797\n",
      "Epoch 360/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6380, val acc: 0.4688\n",
      "Epoch 370/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6376, val acc: 0.4766\n",
      "Epoch 380/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6379, val acc: 0.4719\n",
      "Epoch 390/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6377, val acc: 0.4766\n",
      "Epoch 400/500, LR 0.0001, train loss: 0.6928, train acc: 0.5118, val loss: 0.6378, val acc: 0.4750\n",
      "Epoch 410/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6378, val acc: 0.4750\n",
      "Epoch 420/500, LR 0.0002, train loss: 0.6929, train acc: 0.5118, val loss: 0.6381, val acc: 0.4688\n",
      "Epoch 430/500, LR 0.0001, train loss: 0.6928, train acc: 0.5118, val loss: 0.6376, val acc: 0.4797\n",
      "Epoch 440/500, LR 0.0001, train loss: 0.6929, train acc: 0.5118, val loss: 0.6382, val acc: 0.4672\n",
      "Epoch 450/500, LR 0.0001, train loss: 0.6929, train acc: 0.5118, val loss: 0.6379, val acc: 0.4719\n",
      "Epoch 460/500, LR 0.0001, train loss: 0.6929, train acc: 0.5118, val loss: 0.6381, val acc: 0.4672\n",
      "Epoch 470/500, LR 0.0001, train loss: 0.6929, train acc: 0.5118, val loss: 0.6380, val acc: 0.4688\n",
      "Epoch 480/500, LR 0.0001, train loss: 0.6928, train acc: 0.5118, val loss: 0.6376, val acc: 0.4781\n",
      "Epoch 490/500, LR 0.0001, train loss: 0.6928, train acc: 0.5120, val loss: 0.6382, val acc: 0.4656\n",
      "Epoch 500/500, LR 0.0001, train loss: 0.6928, train acc: 0.5118, val loss: 0.6379, val acc: 0.4734\n",
      "\n",
      "Time total:     1770.15 sec\n",
      "Time per epoch:  3.54 sec\n"
     ]
    }
   ],
   "source": [
    "model = init_model_params(model, random_seed=RANDOM_SEED)\n",
    "model.init_node_embeddings()\n",
    "\n",
    "model = model.to(PARAMS['DEVICE'])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=PARAMS['LR'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "# best_model, history = train_model(dataloaders, dataset_sizes, model, criterion, optimizer, scheduler, PARAMS['N_EPOCHS'], random_seed=random_seed)\n",
    "best_model, history = train_model_2(model, optimizer, scheduler, criterion, dataloaders['train'], dataloaders['val'], N_EPOCHS, RANDOM_SEED, PARAMS['DEVICE'])\n",
    "best_model = best_model.to(PARAMS['DEVICE'])\n",
    "\n",
    "y_preds, y_test = model_predict(best_model, test_loader=dataloaders['test'])\n",
    "\n",
    "cr, cm, auroc = print_classification_report(y_test, y_preds, PARAMS['N_CLASSES'], PARAMS['LABEL_MAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.4973958333333333, 'recall': 1.0, 'f1-score': 0.6643478260869565, 'support': 382}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 386}, 'accuracy': 0.4973958333333333, 'macro avg': {'precision': 0.24869791666666666, 'recall': 0.5, 'f1-score': 0.33217391304347826, 'support': 768}, 'weighted avg': {'precision': 0.24740261501736108, 'recall': 0.4973958333333333, 'f1-score': 0.33044384057971016, 'support': 768}}\n"
     ]
    }
   ],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[382,   0],\n",
       "       [386,   0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84c85347d78e9e1e10700c6d8e0f5eee9b662a9651d925575cf1524e055e4541"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
